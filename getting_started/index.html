<!DOCTYPE html> <html lang=en > <style> #cppn{ position:absolute; top:0; bottom:0; height: 100%; width: 100vw; } iframe { display: block; border-style:none; } </style> <meta property="og:title" content=Flux.jl > <meta property="og:description" content="The elegant machine learning library"> <meta property="og:image" content="/assets/images/FluxGitHubPreview.png"> <meta property="og:url" content="https://fluxml.ai"> <meta name="twitter:title" content=Flux.jl > <meta name="twitter:description" content="The elegant machine learning library"> <meta name="twitter:image" content="/assets/images/FluxGitHubPreview.png"> <meta name="twitter:card" content=summary_large_image > <link rel=apple-touch-icon  sizes=180x180  href="assets/favicon_io/apple-touch-icon.png"> <link rel=icon  type="image/png" sizes=32x32  href="assets/favicon_io/favicon-32x32.png"> <link rel=icon  type="image/png" sizes=16x16  href="assets/favicon_io/favicon-16x16.png"> <link rel=manifest  href="assets/favicon_io/site.webmanifest"> <script async src="https://www.googletagmanager.com/gtag/js?id=UA-36890222-9"></script> <script> window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-36890222-9'); </script> <meta charset=utf-8 > <meta name=viewport  content="width=device-width, initial-scale=1, shrink-to-fit=no"> <link rel=stylesheet  href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta/css/bootstrap.min.css" integrity="sha384-/Y6pD6FV/Vv2HJnA6t+vslU6fwYXjCFtcEpHbNJ0lyAFsXTsjBbfaDjzALeQsN6M" crossorigin=anonymous > <link rel=stylesheet  href="/fluxml-franklin/css/script_default.css"> <link rel=stylesheet  href="/fluxml-franklin/css/site.css"> <link rel=stylesheet  href="https://use.fontawesome.com/releases/v5.6.3/css/all.css" integrity="sha384-UHRtZLI+pbxtHCWp1t77Bi1L4ZtiqrqD80Kn4Z8NTSRyMA2Fd33n5dQ8lWUE00s/" crossorigin=anonymous > <link rel=stylesheet  href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/katex.min.css" integrity="sha384-9eLZqc9ds8eNjO3TmqPeYcDj8n+Qfa4nuSiGYa6DjLNcv9BtN69ZIulL9+8CqC9Y" crossorigin=anonymous > <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/katex.min.js" integrity="sha384-K3vbOmF2BtaVai+Qk37uypf7VrgBubhQreNQe9aGsz9lB63dIFiQVlJbr92dw2Lx" crossorigin=anonymous ></script> <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/contrib/auto-render.min.js" integrity="sha384-kmZOZB5ObwgQnS/DuDg6TScgOiWWBiVt0plIRkZCmE6rDZGrEOQeHM5PcHi+nyqe" crossorigin=anonymous  onload="renderMathInElement(document.body, { delimiters: [ {left: '$[[', right: ']]', display: true}, {left: '\\[', right: '\\]', display: true}, {left: '[[', right: ']]', display: false} ] });"></script> <title>Flux – Elegant ML</title> <nav class="navbar navbar-expand-lg navbar-dark container lighter"> <a class=navbar-brand  href="/fluxml-franklin/"> <div class=logo  style="font-size:30pt;margin-top:-15px;margin-bottom:-10px;">flux</div> </a> <button class=navbar-toggler  type=button  data-toggle=collapse  data-target="#navbarSupportedContent" aria-controls=navbarSupportedContent  aria-expanded=false  aria-label="Toggle navigation"> <span class=navbar-toggler-icon ></span> </button> <div class="collapse navbar-collapse" id=navbarSupportedContent > <ul class="navbar-nav mr-auto"> <li class=nav-item > <a class=nav-link  href="/fluxml-franklin/getting_started/">Getting Started</a> <li class=nav-item > <a class=nav-link  href="https://fluxml.ai/Flux.jl/" target=_blank >Docs</a> <li class=nav-item > <a class=nav-link  href="/fluxml-franklin/blog/">Blog</a> <li class=nav-item > <a class=nav-link  href="/fluxml-franklin/tutorials/">Tutorials</a> <li class=nav-item > <a class=nav-link  href="/fluxml-franklin/ecosystem/">Ecosystem</a> <li class=nav-item > <a class=nav-link  href="/fluxml-franklin/gsoc/">GSoC</a> <li class=nav-item > <a class=nav-link  href="https://discourse.julialang.org/c/domain/ML" target=_blank >Discuss</a> <li class=nav-item > <a class=nav-link  href="https://github.com/FluxML/Flux.jl" target=_blank >GitHub</a> <li class=nav-item > <a class=nav-link  href="https://stackoverflow.com/questions/tagged/flux.jl" target=_blank >Stack Overflow</a> <li class=nav-item > <a class=nav-link  href="https://github.com/FluxML/Flux.jl/blob/master/CONTRIBUTING.md" target=_blank >Contribute</a> </ul> </div> </nav> <div class=franklin-content > <p> </p> <div class=content > <div class=container > <p>Welcome&#33; This section contains information on how to create your first machine learning model using Flux.</p> <p>Flux is 100&#37; pure-Julia stack and provides lightweight abstractions on top of Julia&#39;s native GPU and AD support. It makes the easy things easy while remaining fully hackable. Also, Flux has a next-generation Automatic Differentiation &#40;AD&#41; system <a href="https://github.com/FluxML/Zygote.jl">Zygote</a>.</p> <h2 id=before_you_start ><a href="#before_you_start" class=header-anchor >Before you start</a></h2> <p>Before you begin using Flux, you need to install Julia version 1.3 or later. For more information on installing Julia, see <a href="https://julialang.org/downloads/">Download Julia</a>.</p> <p>After installing Julia, you can install Flux by running the following command in the Julia REPL:</p> <pre><code class="julia hljs">julia&gt; ] add Flux</code></pre>
<br>
<p>Alternatively, you can run the following:</p>
<pre><code class="julia hljs">julia&gt; <span class=hljs-keyword >using</span> Pkg; Pkg.add(<span class=hljs-string >&quot;Flux&quot;</span>)</code></pre>
<br>
<p>Flux provides GPU support. For more information on obtaining GPU support, see <a href="https://github.com/JuliaGPU/CUDA.jl">CUDA.jl</a> and <a href="https://fluxml.ai/Flux.jl/stable/gpu/">Flux documentation on GPU support</a>.</p>
<br>
<h2 id=getting_help ><a href="#getting_help" class=header-anchor >Getting Help</a></h2>
<p>If you run into any issues on your journey learning Flux.jl, please post on Stack Overflow under the <a href="https://stackoverflow.com/questions/tagged/flux.jl">Flux.jl tag</a> or ask a question on the <a href="https://discourse.julialang.org/c/domain/ml/">Julia Discourse under the Machine Learning domain</a>.</p>
<h2 id=create_your_first_model ><a href="#create_your_first_model" class=header-anchor >Create your first model </a></h2>
<p>In this tutorial, you&#39;ll create your first machine learning model using Flux. This is a simple linear regression model that attempts to recover a linear function by looking at noisy examples.</p>
<br>
<h3 id=step_1_import_flux ><a href="#step_1_import_flux" class=header-anchor >Step 1: Import Flux</a></h3>
<p>To import Flux add the following:</p>
<pre><code class="julia hljs"><span class=hljs-keyword >using</span> Flux</code></pre>
<br>
<h3 id=step_2_create_the_training_data ><a href="#step_2_create_the_training_data" class=header-anchor >Step 2: Create the training data</a></h3>
<p>First, we&#39;ll write a function that generates our &quot;true&quot; data. We&#39;ll use to use Flux to recover <code>W_truth</code> and <code>b_truth</code> by looking only at examples of the <code>ground_truth</code> function.</p>
<pre><code class="julia hljs">W_truth = [<span class=hljs-number >1</span> <span class=hljs-number >2</span> <span class=hljs-number >3</span> <span class=hljs-number >4</span> <span class=hljs-number >5</span>;
            <span class=hljs-number >5</span> <span class=hljs-number >4</span> <span class=hljs-number >3</span> <span class=hljs-number >2</span> <span class=hljs-number >1</span>]
b_truth = [-<span class=hljs-number >1.0</span>; -<span class=hljs-number >2.0</span>]
ground_truth(x) = W_truth*x .+ b_truth</code></pre>
<br>
<p>Next, we generate our training data by passing random vectors into the ground truth function. We&#39;ll also add Gaussian noise using <code>randn&#40;&#41;</code> so that it&#39;s not <em>too</em> easy for Flux to figure out the model.</p>
<pre><code class="julia hljs">x_train = [ <span class=hljs-number >5</span> .* rand(<span class=hljs-number >5</span>) <span class=hljs-keyword >for</span> _ <span class=hljs-keyword >in</span> <span class=hljs-number >1</span>:<span class=hljs-number >10_000</span> ]
y_train = [ ground_truth(x) + <span class=hljs-number >0.2</span> .* randn(<span class=hljs-number >2</span>) <span class=hljs-keyword >for</span> x <span class=hljs-keyword >in</span> x_train ]</code></pre>
<p>There are two important things to note in this example which differ from real machine learning problems:</p>
<ul>
<li><p>Our variables are individual vectors, stored inside another vector. Usually,  we would have a collection of N-dimensional arrays &#40;N &gt;&#61; 2&#41; as our data.</p>

<li><p>In a real learning scenario, we would not have access to our ground truth, only the training examples.</p>

</ul>
<h3 id=step_3_define_your_model ><a href="#step_3_define_your_model" class=header-anchor >Step 3: Define your model</a></h3>
<p>Next, we define the model we want to use to learn the data. We&#39;ll use the same form that we used for our training data:</p>
<pre><code class="julia hljs">model(x) = W*x .+ b</code></pre>
<br>
<p>We need to set the parameters of the model &#40;<code>W</code> and <code>b</code>&#41; to some initial values. It&#39;s fairly common to use random values, so we&#39;ll do that:</p>
<pre><code class="julia hljs">W = rand(<span class=hljs-number >2</span>, <span class=hljs-number >5</span>)
b = rand(<span class=hljs-number >2</span>)</code></pre>
<br>
<p>You can learn more about defining models in this video:</p>

<iframe width="100%" height=450  style="display:block" src="https://www.youtube.com/embed/XrAUGRX998E" title="YouTube video player" frameborder=0  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

<br>
<h3 id=step_4_define_a_loss_function ><a href="#step_4_define_a_loss_function" class=header-anchor >Step 4: Define a loss function</a></h3>
<p>A loss function evaluates a machine learning model&#39;s performance. In other words, it measures how far the model is from its target prediction. Flux lets you define your own custom loss function, or you can use one of the <a href="https://fluxml.ai/Flux.jl/stable/training/training/#Loss-Functions-1">Loss Functions</a> that Flux provides. </p>
<p>For this example, we&#39;ll define a loss function that measures the squared distance from the predicted output to the actual output:</p>
<pre><code class="julia hljs"><span class=hljs-keyword >function</span> loss(x, y)
  ŷ = model(x)
  sum((y .- ŷ).^<span class=hljs-number >2</span>)
<span class=hljs-keyword >end</span></code></pre>
<br>
<h3 id=step_5_set_an_optimiser ><a href="#step_5_set_an_optimiser" class=header-anchor >Step 5: Set an optimiser</a></h3>
<p>You train a machine learning model by running an optimization algorithm &#40;optimiser&#41; that finds the best parameters &#40;<code>W</code> and <code>b</code>&#41;. The best parameters for a model are the ones that achieve the best score of the <code>loss</code> function. Flux provides <a href="https://fluxml.ai/Flux.jl/stable/training/optimisers/">Optimisers</a> that you can use to train a model. </p>
<p>For this tutorial, we&#39;ll use a classic gradient descent optimiser with learning rate η &#61; 0.01:</p>
<pre><code class="julia hljs">opt = Descent(<span class=hljs-number >0.01</span>)</code></pre>
<br>
<h3 id=step_6_train_your_model ><a href="#step_6_train_your_model" class=header-anchor >Step 6: Train your model</a></h3>
<p>Training a model is the process of computing the gradients with respect to the parameters for each input in the data. At every step, the optimiser updates all of the parameters until it finds a good value for them. This process can be written as a loop: we iterate over the examples in <code>x_train</code> and <code>y_train</code> and update the model for each example.</p>
<p>To indicate that we want all derivatives of <code>W</code> and <code>b</code>, we write <code>ps &#61; params&#40;W, b&#41;</code>. This is a convenience function that Flux provides so that we don&#39;t have to explicitly list every gradient we want. Check out the section on <a href="https://fluxml.ai/Flux.jl/stable/models/basics/#Taking-Gradients">Taking Gradients</a> if you want to learn more about how this works.</p>
<p>We can now execute the training procedure for our model:</p>
<pre><code class="julia hljs">train_data = zip(x_train, y_train)
ps = params(W, b)

<span class=hljs-keyword >for</span> (x,y) <span class=hljs-keyword >in</span> train_data
  gs = Flux.gradient(ps) <span class=hljs-keyword >do</span>
    loss(x,y)
  <span class=hljs-keyword >end</span>
  Flux.Optimise.update!(opt, ps, gs)
<span class=hljs-keyword >end</span></code></pre>
<br>
<blockquote>
<p><strong>Note:</strong> With this pattern, it is easy to add more complex learning routines that make use of control flow, distributed compute, scheduling optimisations, etc. Note that the pattern above is a simple Julia <em>for loop</em> but it could also be replaced with a <em>while loop</em>.</p>
</blockquote>
<br>
<p>While writing your own loop is powerful, sometimes you just want to do the simple thing without writing too much code. Flux lets you do this with <a href="https://fluxml.ai/Flux.jl/stable/training/training/#Training-1">Flux.train&#33;</a>, which runs one training epoch over a dataset. <code>Flux.train&#33;</code> computes gradients and updates model parameters for every sample or batch of samples. In our case, we could have replaced the above loop with the following statement:</p>
<pre><code class="julia hljs">Flux.train!(loss, params(W, b), train_data, opt)</code></pre>
<br>
<p>For more ways to train a model in Flux, see <a href="https://fluxml.ai/Flux.jl/stable/training/training/#Training-1">Training</a>.</p>
<br>
<h3 id=step_7_examine_the_results ><a href="#step_7_examine_the_results" class=header-anchor >Step 7: Examine the Results</a></h3>
<p>The training loop we ran modified <code>W</code> and <code>b</code> to be closer to the values used to generate the training data &#40;<code>W</code> and <code>b</code>&#41;. We can see how well we did by printing out the difference between the learned and actual matrices.</p>
<pre><code class="julia hljs"><span class=hljs-meta >@show</span> W
<span class=hljs-meta >@show</span> maximum(abs, W .- W_truth)</code></pre>
<p>Because the data and initialization are random, your results may vary slightly, but in most cases, the largest difference between the elements of learned and actual <code>W</code> matrix is no more than 4&#37;.</p>
<h3 id=step_8_run_the_script ><a href="#step_8_run_the_script" class=header-anchor >Step 8: Run the script</a></h3>
<p>Finally, create a file with extension <code>.jl</code> with the code above in any IDE and run it as <code>julia name-of-your-file.jl</code>. You can use the <a href="https://www.julia-vscode.org/">Julia VSCode extension</a> to edit and run Julia code. Alternatively, you can run Julia code on a Jupyter notebook &#40;see <a href="https://github.com/JuliaLang/IJulia.jl">IJulia</a>&#41;. Here is the full version of the code:</p>
<pre><code class="julia hljs"><span class=hljs-keyword >using</span> Flux

<span class=hljs-comment ># Define the ground truth model. We aim to recover W_truth and b_truth using</span>
<span class=hljs-comment ># only examples of ground_truth()</span>
W_truth = [<span class=hljs-number >1</span> <span class=hljs-number >2</span> <span class=hljs-number >3</span> <span class=hljs-number >4</span> <span class=hljs-number >5</span>;
            <span class=hljs-number >5</span> <span class=hljs-number >4</span> <span class=hljs-number >3</span> <span class=hljs-number >2</span> <span class=hljs-number >1</span>]
b_truth = [-<span class=hljs-number >1.0</span>; -<span class=hljs-number >2.0</span>]
ground_truth(x) = W_truth*x .+ b_truth

<span class=hljs-comment ># Generate the ground truth training data as vectors-of-vectors</span>
x_train = [ <span class=hljs-number >5</span> .* rand(<span class=hljs-number >5</span>) <span class=hljs-keyword >for</span> _ <span class=hljs-keyword >in</span> <span class=hljs-number >1</span>:<span class=hljs-number >10_000</span> ]
y_train = [ ground_truth(x) + <span class=hljs-number >0.2</span> .* randn(<span class=hljs-number >2</span>) <span class=hljs-keyword >for</span> x <span class=hljs-keyword >in</span> x_train ]

<span class=hljs-comment ># Define and initialize the model we want to train</span>
model(x) = W*x .+ b
W = rand(<span class=hljs-number >2</span>, <span class=hljs-number >5</span>)
b = rand(<span class=hljs-number >2</span>)

<span class=hljs-comment ># Define pieces we need to train: loss function, optimiser, examples, and params</span>
<span class=hljs-keyword >function</span> loss(x, y)
  ŷ = model(x)
  sum((y .- ŷ).^<span class=hljs-number >2</span>)
<span class=hljs-keyword >end</span>
opt = Descent(<span class=hljs-number >0.01</span>)
train_data = zip(x_train, y_train)
ps = params(W, b)

<span class=hljs-comment ># Execute a training epoch</span>
<span class=hljs-keyword >for</span> (x,y) <span class=hljs-keyword >in</span> train_data
  gs = gradient(ps) <span class=hljs-keyword >do</span>
    loss(x,y)
  <span class=hljs-keyword >end</span>
  Flux.Optimise.update!(opt, ps, gs)
<span class=hljs-keyword >end</span>

<span class=hljs-comment ># An alternate way to execute a training epoch</span>
<span class=hljs-comment ># Flux.train!(loss, params(W, b), train_data, opt)</span>

<span class=hljs-comment ># Print out how well we did</span>
<span class=hljs-meta >@show</span> W
<span class=hljs-meta >@show</span> maximum(abs, W .- W_truth)</code></pre>
<br>
<h2 id=whats_next ><a href="#whats_next" class=header-anchor >What&#39;s next</a></h2>
<p>Congratulations&#33; You have created and trained a model using Flux. Now, you can continue exploring Flux&#39;s capabilities:</p>
<ul>
<li><p><a href="tutorials/2020/09/15/deep-learning-flux.html">60-minute blitz tutorial</a> is a quick intro to Flux loosely based on <a href="https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html">PyTorch&#39;s tutorial</a>.</p>

<li><p><a href="https://github.com/FluxML/model-zoo">Flux Model Zoo</a> contains various demonstrations of Flux. </p>

<li><p><a href="https://juliaacademy.com/">JuliaAcademy</a> offers introductory courses to Julia and Flux.</p>

<li><p><a href="https://fluxml.ai/Flux.jl/stable/">Flux&#39;s official documentation</a>.</p>

</ul>
<p>As you continue to progress through your Flux and Julia journey, please feel free to share it on <a href="https://twitter.com/FluxML">Twitter and tag us</a>, we would love to see what awesome things the #FluxML community is up to.</p>

  </div>
</div>

<!-- <div class=page-foot >
  <div class=copyright >
    &copy; {{ fill author }}. {{isnotpage /tag/*}}Last modified: {{ fill fd_mtime }}.{{end}} Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>.
  </div>
</div> -->
</div>

    
    <div class="container footer lighter">
      <p>Flux: A Deep Learning Library for the Julia Programming Language
</p>
      <a href="https://twitter.com/FluxML?ref_src=twsrc%5Etfw" class=twitter-follow-button  data-show-count=false >Follow @FluxML</a>
      <script async src="https://platform.twitter.com/widgets.js" charset=utf-8 ></script>
    </div>

    
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/js/bootstrap.min.js"></script>
    
    
    
    <script src="/fluxml-franklin//instant.page/1.0.0" type=module  integrity="sha384-6w2SekMzCkuMQ9sEbq0cLviD/yR2HfA/+ekmKiBnFlsoSvb/VmQFSi/umVShadQI"></script>