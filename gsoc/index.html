<!DOCTYPE html> <html lang=en > <style> #cppn{ position:absolute; top:0; bottom:0; height: 100%; width: 100vw; } iframe { display: block; border-style:none; } </style> <meta property="og:title" content=Flux.jl > <meta property="og:description" content="The elegant machine learning library"> <meta property="og:image" content="/assets/images/FluxGitHubPreview.png"> <meta property="og:url" content="https://fluxml.ai"> <meta name="twitter:title" content=Flux.jl > <meta name="twitter:description" content="The elegant machine learning library"> <meta name="twitter:image" content="/assets/images/FluxGitHubPreview.png"> <meta name="twitter:card" content=summary_large_image > <link rel=apple-touch-icon  sizes=180x180  href="assets/favicon_io/apple-touch-icon.png"> <link rel=icon  type="image/png" sizes=32x32  href="assets/favicon_io/favicon-32x32.png"> <link rel=icon  type="image/png" sizes=16x16  href="assets/favicon_io/favicon-16x16.png"> <link rel=manifest  href="assets/favicon_io/site.webmanifest"> <script async src="https://www.googletagmanager.com/gtag/js?id=UA-36890222-9"></script> <script> window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-36890222-9'); </script> <meta charset=utf-8 > <meta name=viewport  content="width=device-width, initial-scale=1, shrink-to-fit=no"> <link rel=stylesheet  href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta/css/bootstrap.min.css" integrity="sha384-/Y6pD6FV/Vv2HJnA6t+vslU6fwYXjCFtcEpHbNJ0lyAFsXTsjBbfaDjzALeQsN6M" crossorigin=anonymous > <link rel=stylesheet  href="/fluxml-franklin/css/script_default.css"> <link rel=stylesheet  href="/fluxml-franklin/css/site.css"> <link rel=stylesheet  href="https://use.fontawesome.com/releases/v5.6.3/css/all.css" integrity="sha384-UHRtZLI+pbxtHCWp1t77Bi1L4ZtiqrqD80Kn4Z8NTSRyMA2Fd33n5dQ8lWUE00s/" crossorigin=anonymous > <link rel=stylesheet  href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/katex.min.css" integrity="sha384-9eLZqc9ds8eNjO3TmqPeYcDj8n+Qfa4nuSiGYa6DjLNcv9BtN69ZIulL9+8CqC9Y" crossorigin=anonymous > <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/katex.min.js" integrity="sha384-K3vbOmF2BtaVai+Qk37uypf7VrgBubhQreNQe9aGsz9lB63dIFiQVlJbr92dw2Lx" crossorigin=anonymous ></script> <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/contrib/auto-render.min.js" integrity="sha384-kmZOZB5ObwgQnS/DuDg6TScgOiWWBiVt0plIRkZCmE6rDZGrEOQeHM5PcHi+nyqe" crossorigin=anonymous  onload="renderMathInElement(document.body, { delimiters: [ {left: '$[[', right: ']]', display: true}, {left: '\\[', right: '\\]', display: true}, {left: '[[', right: ']]', display: false} ] });"></script> <title>Flux – Elegant ML</title> <nav class="navbar navbar-expand-lg navbar-dark container lighter"> <a class=navbar-brand  href="/fluxml-franklin/"> <div class=logo  style="font-size:30pt;margin-top:-15px;margin-bottom:-10px;">flux</div> </a> <button class=navbar-toggler  type=button  data-toggle=collapse  data-target="#navbarSupportedContent" aria-controls=navbarSupportedContent  aria-expanded=false  aria-label="Toggle navigation"> <span class=navbar-toggler-icon ></span> </button> <div class="collapse navbar-collapse" id=navbarSupportedContent > <ul class="navbar-nav mr-auto"> <li class=nav-item > <a class=nav-link  href="/fluxml-franklin/getting_started/">Getting Started</a> <li class=nav-item > <a class=nav-link  href="https://fluxml.ai/Flux.jl/" target=_blank >Docs</a> <li class=nav-item > <a class=nav-link  href="/fluxml-franklin/blog/">Blog</a> <li class=nav-item > <a class=nav-link  href="/fluxml-franklin/tutorials/">Tutorials</a> <li class=nav-item > <a class=nav-link  href="/fluxml-franklin/ecosystem/">Ecosystem</a> <li class=nav-item > <a class=nav-link  href="/fluxml-franklin/gsoc/">GSoC</a> <li class=nav-item > <a class=nav-link  href="https://discourse.julialang.org/c/domain/ML" target=_blank >Discuss</a> <li class=nav-item > <a class=nav-link  href="https://github.com/FluxML/Flux.jl" target=_blank >GitHub</a> <li class=nav-item > <a class=nav-link  href="https://stackoverflow.com/questions/tagged/flux.jl" target=_blank >Stack Overflow</a> <li class=nav-item > <a class=nav-link  href="https://github.com/FluxML/Flux.jl/blob/master/CONTRIBUTING.md" target=_blank >Contribute</a> </ul> </div> </nav> <div class=franklin-content > <p> </p> <div class=content > <div class=container > <h1 id=google_summer_of_code_projects ><a href="#google_summer_of_code_projects" class=header-anchor >Google Summer of Code Projects</a></h1> <p>Flux usually takes part in <a href="https://summerofcode.withgoogle.com">Google Summer of Code</a>, as part of the wider Julia organisation. We follow the same <a href="https://julialang.org/jsoc/guidelines/">rules and application guidelines</a> as Julia, so please check there for more information on applying. Below are a set of ideas for potential projects &#40;though you are welcome to explore anything you are interested in&#41;.</p> <p>Flux projects are typically very competitive; we encourage you to get started early, as successful students typically have early PRs or working prototypes as part of the application. It is a good idea to simply start contributing via issue discussion and PRs and let a project grow from there; you can take a look at <a href="https://github.com/issues?utf8&#61;✓&amp;q&#61;is&#37;3Aopen&#43;archived&#37;3Afalse&#43;user&#37;3AFluxML&#43;label&#37;3A&#37;22help&#43;wanted&#37;22">this list of issues</a> for some starter contributions.</p> <h3 id=port_ml_tutorials ><a href="#port_ml_tutorials" class=header-anchor >Port ML Tutorials</a></h3> <p>There are many high-quality open-source tutorials and learning materials available, for example from PyTorch and fast.ai. We&#39;d like to have Flux ports of these that we can add to the model zoo, and eventually publish to the Flux website.</p> <p>Mentors: <a href="https://github.com/DhairyaLGandhi/">Dhairya Gandhi</a>.</p> <h3 id=ferminets_generative_synthesis_for_automating_the_choice_of_neural_architectures ><a href="#ferminets_generative_synthesis_for_automating_the_choice_of_neural_architectures" class=header-anchor >FermiNets: Generative Synthesis for Automating the Choice of Neural Architectures</a></h3> <p>The application of machine learning requires an understanding a practictioner to optimize a neural architecture for a given problem, or does it? Recently techniques in automated machine learning, also known as AutoML, have dropped this requirement by allowing for good architectures to be found automatically. One such method is the <a href="https://arxiv.org/abs/1809.05989">FermiNet</a> which employs generative synthesis to give a neural architecture which respects certain operational requirements. The goal of this project is to implement the FermiNet in Flux to allow for automated sythesis of neural networks.</p> <p>Mentors: <a href="https://github.com/ChrisRackauckas">Chris Rackauckas</a> and <a href="https://github.com/DhairyaLGandhi/">Dhairya Gandhi</a>.</p> <h3 id=differentiable_rendering_hard ><a href="#differentiable_rendering_hard" class=header-anchor >Differentiable Rendering &#91;HARD&#93;</a></h3> <p>Expected Outcome: This is motivated to create SoftRasterizer/DiB-R based projects. We already have RayTracer.jl which is motivated by OpenDR. &#40;Of course, if someone wants to implement NERF - like models they are most welcome to submit a proposal&#41;. We would ideally target at least 2 of these models.</p> <p>Skills: GPU Programming, Deep Learning, &#40;deep&#41; familiarity with the literature, familiarity with defining &#40;a lot of&#41; Custom Adjoints</p> <p>Mentors: <a href="https://github.com/DhairyaLGandhi/">Dhairya Gandhi</a>, <a href="https://github.com/avik-pal">Avik Pal</a></p> <h3 id=core_development_medium ><a href="#core_development_medium" class=header-anchor >Core Development &#91;MEDIUM&#93;</a></h3> <p>Expected Outcomes:</p> <ul> <li><p>Some of the functions require custom adjoints for speedup</p> <li><p>Functions require GPU kernels. Some of these are of common interest to the community like – knn, etc.</p> <li><p>Benchmarking with Tensorflow Graphics and Pytorch3D. We already have the scripts for kaolin, need to extend that.</p> <li><p>Most of these problems are listed as issues in the main repo.</p> </ul> <p>Skills: GPU Programming, Deep Learning, familiarity with defining &#40;a lot of&#41; Custom Adjoints</p> <p>Mentors: <a href="https://github.com/DhairyaLGandhi/">Dhairya Gandhi</a></p> <h3 id=fastaijl_development ><a href="#fastaijl_development" class=header-anchor >FastAI.jl Development</a></h3> <p><strong>Difficulty:</strong> Medium</p> <p>In this project, you will assist the <a href="https://julialang.zulipchat.com/#narrow/stream/237432-ml-ecosystem-coordination">ML community team</a> with building FastAI.jl on top of the existing JuliaML &#43; FluxML ecosystem packages. The primary goal is to create an equivalent to docs.fast.ai. This will require building the APIs, documenting them, and creating the appropriate tutorials. Some familiarity with the following Julia packages is preferred, but it is not required:</p> <ul> <li><p><a href="https://github.com/JuliaML/MLDataPattern.jl.git">MLDataPattern.jl</a></p> <li><p><a href="https://github.com/lorenzoh/FluxTraining.jl.git">FluxTraining.jl</a></p> <li><p><a href="https://github.com/lorenzoh/DataAugmentation.jl">DataAugmentation.jl</a></p> </ul> <p>A stretch goal can include extending FastAI.jl beyond its Python-equivalent by leveraging the flexibility in the underlying Julia packages. For example, creating and designing abstractions for distributed data parallel training.</p> <p><strong>Skills:</strong> Familiarity with deep learning pipelines, common practices, Flux.jl, and MLDataPattern.jl</p> <p>Mentors: <a href="https://github.com/darsnack">Kyle Daruwalla</a></p> <h3 id=differentiable_computer_vision_hard ><a href="#differentiable_computer_vision_hard" class=header-anchor >Differentiable Computer Vision &#91;HARD&#93;</a></h3> <p>Expected Outcome:</p> <p>Create a library of utliity functions that can consume Julia&#39;s Imaging libraries to make them differentiable. With Zygote.jl, we have the platform to take a general purpose package and apply automatic differentiation to it. This project is motivated to use existing libraries that offer perform computer vision tasks, and augment them with AD to perform tasks such as homography regression.</p> <p>Skills: Familiarity with automatic differentiation, deep learning, and defining &#40;a lot of&#41; Custom Adjoints</p> <p>Mentors: <a href="https://github.com/DhairyaLGandhi/">Dhairya Gandhi</a></p> <h3 id=make_the_model-zoo_robust ><a href="#make_the_model-zoo_robust" class=header-anchor >Make the Model-Zoo Robust</a></h3> <p>The model zoo has been a great resource for folks starting with Flux and machine learning in general. This time, we want to make the zoo significantly more useful. With Artifacts, we can have the models be completely reusable, and have high quality trained models availiable trivially. This would involve training the existing models and making the weights available through artifacts. We would also want to have ways to verify that the models perform well on the test datasets. For some more insight, we are looking out for more <a href="https://github.com/FluxML/Metalhead.jl">Metalhead.jl</a> kind of workflows with a reference to how to use them in the zoo, plus finding high quality pretrained weights for the models, that have been verified to give the expected results, including numerical debugging.</p> <p>Mentors: <a href="https://github.com/dhairyagandhi96/">Dhairya Gandhi</a>, <a href="https://github.com/staticfloat">Elliot Saba</a>.</p> <h3 id=cuda_hacking ><a href="#cuda_hacking" class=header-anchor >CUDA Hacking</a></h3> <p>Are you a performance nut? Help us implement cutting-edge CUDA kernels in Julia for operations important across deep learning, scientific computing and more. We also need help developing our wrappers for machine learning, sparse matrices and more, as well as CI and infrastructure. Contact us to develop a project plan.</p> <p>Mentors: <a href="https://github.com/maleadt">Tim Besard</a>, <a href="https://github.com/MikeInnes">Mike Innes</a>.</p> <h3 id=reinforcement_learning_environments ><a href="#reinforcement_learning_environments" class=header-anchor >Reinforcement Learning Environments</a></h3> <p>Develop a series of reinforcement learning environments, in the spirit of the <a href="https://gym.openai.com">OpenAI Gym</a>. Although we have wrappers for the gym available, it is hard to install &#40;due to the Python dependency&#41; and, since it&#39;s written in Python and C code, we can&#39;t do more interesting things with it &#40;such as differentiate through the environments&#41;. A pure-Julia version that supports a similar API and visualisation options would be valuable to anyone doing RL with Flux.</p> <p>Mentors: <a href="https://github.com/DhairyaLGandhi/">Dhairya Gandhi</a>, <a href="https://github.com/staticfloat">Elliot Saba</a>.</p> <h3 id=model_zoo_on_tpu ><a href="#model_zoo_on_tpu" class=header-anchor >Model Zoo on TPU</a></h3> <p>Julia has experimental support for executing code on <a href="https://github.com/JuliaTPU/XLA.jl">TPUs</a> TPUs enable training cutting edge machine larning models written using Flux. However, TPUs are not able to execute arbitrary code and thus often require individual attention to fix new patterns in XLA.jl or other packages. Additionally, the performance characteristics of the TPU hardware are quite unlike that of CPU or even GPU and models may thus require TPU-specific adjustments to achieve peak performance. Lastly, the speed of TPUs presents signficant challenges to data input pipelines even at single-TPU levels of performance. Having a wide set of models available that are tuned for TPU will aid in finding common abstractions for models independent of hardware.</p> <p>Mentors: <a href="https://github.com/Keno">Keno Fischer</a></p> <h3 id=benchmarks ><a href="#benchmarks" class=header-anchor >Benchmarks</a></h3> <p>A benchmark suite would help us to keep Julia&#39;s performance for ML models in shape, as well as revealing opportunities for improvement. Like the model-zoo project, this would involve contributing standard models that exercise common ML use case &#40;images, text etc&#41; and profiling them. The project could extend to include improving performance where possible, or creating a &quot;benchmarking CI&quot; like Julia&#39;s own <a href="https://github.com/JuliaCI/Nanosoldier.jl">nanosoldier</a>.</p> <p>Mentors: <a href="https://github.com/DhairyaLGandhi/">Dhairya Gandhi</a>, <a href="https://github.com/staticfloat">Elliot Saba</a>.</p> <h3 id=multi-gpu_training ><a href="#multi-gpu_training" class=header-anchor >Multi-GPU training</a></h3> <p>Implement and demonstrate multi-GPU parallelism. One route is to expose communication primitives from NVIDIA&#39;s <a href="https://developer.nvidia.com/nccl">NCCL</a> library and use these to build tooling for model parallelism and distributed training. The project should demonstrate parallel training of a Flux model with benchmarks.</p> <p>Mentors: <a href="https://github.com/vchuravy">Valentin Churavy</a>, <a href="https://github.com/maleadt">Tim Besard</a></p> <h3 id=distributed_training ><a href="#distributed_training" class=header-anchor >Distributed Training</a></h3> <p>Add a distributed training API to Flux, possibly inspired by PyTorch&#39;s equivalent. Any distributed training algorithm could be used &#40;ideally the foundations make it easy to experiment with different setups&#41;, though the easiest is likely to implement an MXNet-like parameter server. It should demonstrate training a Flux model with data distributed over multiple nodes, with benchmarks.</p> <p>Mentors: <a href="https://github.com/vchuravy">Valentin Churavy</a>, <a href="https://github.com/maleadt">Tim Besard</a></p> <h3 id=sparse_gpu_and_ml_support ><a href="#sparse_gpu_and_ml_support" class=header-anchor >Sparse GPU and ML support</a></h3> <p>While Julia supports dense GPU arrays well via <a href="https://github.com/JuliaGPU/CUDA.jl">CUDA.jl</a>, we lack up-to-date wrappers for sparse operations. This project would involve wrapping CUDA&#39;s sparse support, with <a href="https://github.com/JuliaGPU/CUSPARSE.jl">CUSPARSE.jl</a> as a starting point, adding them to CUDA.jl, and perhaps demonstrating their use via a sparse machine learning model.</p> <h3 id=nlp_tools_and_models ><a href="#nlp_tools_and_models" class=header-anchor >NLP Tools and Models</a></h3> <p>Build deep learning models for Natural Language Processing in Julia. <a href="https://github.com/juliatext/TextAnalysis.jl">TextAnalysis</a> and <a href="https://github.com/JuliaText/WordTokenizers.jl">WordTokenizers</a> contains the basic algorithms and data structures to work with textual data in Julia. On top of that base, we want to build modern deep learning models based on recent research. The following tasks can span multiple students and projects.</p> <p>It is important to note that we want practical, usable solutions to be created, not just research models. This implies that a large part of the effort will need to be in finding and using training data, and testing the models over a wide variety of domains. Pre-trained models must be available to users, who should be able to start using these without supplying their own training data.</p> <ul> <li><p>Implement <a href="https://allennlp.org/elmo">ELMo</a> in Julia</p> <li><p>Implement <a href="https://ai.googleblog.com/2019/12/albert-lite-bert-for-self-supervised.html">ALBERT</a> in Julia</p> <li><p>Implement GPT/GPT-2 in Julia</p> <li><p>Implement <a href="https://arxiv.org/abs/1909.03186">extractive summarisation based on Transformers</a></p> <li><p>Implement practical models for</p> <ul> <li><p>Dependency Tree Parsing</p> <li><p>Morphological extractions</p> <li><p>Translations &#40;using Transformers&#41;</p> </ul> <li><p>Indic language support – validate and test all models for Indic languages</p> <ul> <li><p>ULMFiT models for Indic languages</p> </ul> <li><p>Chinese tokenisation and parsing</p> </ul> <p><strong>Mentors</strong>: <a href="https://github.com/aviks/">Avik Sengupta</a></p> <h3 id=automated_music_generation ><a href="#automated_music_generation" class=header-anchor >Automated music generation</a></h3> <p>Neural network based models can be used for music analysis and music generation &#40;composition&#41;. A suite of tools in Julia to enable research in this area would be useful. This is a large, complex project that is suited for someone with an interest in music and machine learning. This project will need a mechanism to read music files &#40;primarily MIDI&#41;, a way to synthesise sounds, and finally a model to learn composition. All of this is admittedly a lot of work, so the exact boundaries of the project can be flexible, but this can be an exciting project if you are interested in both music and machine learning.</p> <p><strong>Recommended Skills</strong>: Music notation, some basic music theory, MIDI format, Transformer and LSTM architectures</p> <p><strong>Resources</strong>: <a href="https://magenta.tensorflow.org/music-transformer">Music Transformer</a>, <a href="https://magenta.tensorflow.org/maestro-wave2midi2wave">Wave2MIDI2Wave</a></p> <p><strong>Mentors</strong>: <a href="https://github.com/aviks/">Avik Sengupta</a></p> <h3 id=neural_networks_for_solving_differential_equations ><a href="#neural_networks_for_solving_differential_equations" class=header-anchor >Neural networks for solving differential equations</a></h3> <p>Neural networks can be used as a method for efficiently solving difficult partial differential equations. Efficient implementations from recent papers are being explored as part of the <a href="https://github.com/JuliaDiffEq/NeuralNetDiffEq.jl">NeuralNetDiffEq.jl</a> package. The <a href="https://github.com/JuliaDiffEq/NeuralNetDiffEq.jl/issues">issue tracker</a> contains links to papers which would be interesting new neural network based methods to implement and benchmark against classical techniques.</p> <p><strong>Recommended Skills</strong>: Background knowledge in numerical analysis and machine learning.</p> <p><strong>Expected Results</strong>: New neural network based solver methods.</p> <p><strong>Mentors</strong>: <a href="https://github.com/ChrisRackauckas">Chris Rackauckas</a></p> <h3 id=accelerating_optimization_via_machine_learning_with_surrogate_models ><a href="#accelerating_optimization_via_machine_learning_with_surrogate_models" class=header-anchor >Accelerating optimization via machine learning with surrogate models</a></h3> <p>In many cases, when attempting to optimize a function <code>f&#40;p&#41;</code> each calculation of <code>f</code> is very expensive. For example, evaluating <code>f</code> may require solving a PDE or other applications of complex linear algebra. Thus, instead of always directly evaluating <code>f</code>, one can develop a surrogate model <code>g</code> which is approximately <code>f</code> by training on previous data collected from <code>f</code> evaluations. This technique of using a trained surrogate in place of the real function is called surrogate optimization and mixes techniques from machine learning to accelerate optimization.</p> <p>Advanced techniques <a href="https://www.cambridge.org/core/journals/acta-numerica/article/kernel-techniques-from-machine-learning-to-meshless-methods/00686923110F799A1537C4F02BBAAE8E">utilize radial basis functions</a> and Gaussian processes in order to interpolate to new parameters to estimate <code>f</code> in areas which have not been sampled. <a href="http://www.ressources-actuarielles.net/EXT/ISFA/1226.nsf/9c8e3fd4d8874d60c1257052003eced6/e7dc33e4da12c5a9c12576d8002e442b/\&#36;FILE/Jones01.pdf">Adaptive training techniques</a> explore how to pick new areas to evaluate <code>f</code> to better hone in on global optima. The purpose of this project is to explore these techniques and build a package which performs surrogate optimizations.</p> <p><strong>Recommended Skills</strong>: Background knowledge of standard machine learning, statistical, or optimization techniques. Strong knowledge of numerical analysis is helpful but not required.</p> <p><strong>Expected Results</strong>: Library functions for performing surrogate optimization with tests on differential equation models.</p> <p><strong>Mentors</strong>: <a href="https://github.com/ChrisRackauckas">Chris Rackauckas</a></p> <h3 id=parameter_estimation_for_nonlinear_dynamical_models ><a href="#parameter_estimation_for_nonlinear_dynamical_models" class=header-anchor >Parameter estimation for nonlinear dynamical models</a></h3> <p>Machine learning has become a popular tool for understanding data, but scientists typically understand the world through the lens of physical laws and their resulting dynamical models. These models are generally differential equations given by physical first principles, where the constants in the equations such as chemical reaction rates and planetary masses determine the overall dynamics. The inverse problem to simulation, known as parameter estimation, is the process of utilizing data to determine these model parameters.</p> <p>The purpose of this project is to utilize the growing array of statistical, optimization, and machine learning tools in the Julia ecosystem to build library functions that make it easy for scientists to perform this parameter estimation with the most high-powered and robust methodologies. Possible projects include improving methods for Bayesian estimation of parameters via Stan.jl and Julia-based libraries like Turing.jl, or global optimization-based approaches. Novel techniques like classifying model outcomes via support vector machines and deep neural networks is can also be considered. Research and benchmarking to attempt to find the most robust methods will take place in this project. Additionally, the implementation of methods for estimating structure, such as <a href="http://www.pnas.org/content/111/52/18507">topological sensitivity analysis</a> along with performance enhancements to existing methods will be considered.</p> <p>Some work in this area can be found in <a href="https://github.com/JuliaDiffEq/DiffEqParamEstim.jl">DiffEqParamEstim.jl</a> and <a href="https://github.com/JuliaDiffEq/DiffEqBayes.jl">DiffEqBayes.jl</a>. Examples can be found <a href="http://docs.juliadiffeq.org/latest/analysis/parameter_estimation.html">in the DifferentialEquations.jl documentation</a>.</p> <p><strong>Recommended Skills</strong>: Background knowledge of standard machine learning, statistical, or optimization techniques. It&#39;s recommended but not required that one has basic knowledge of differential equations and DifferentialEquations.jl. Using the differential equation solver to get outputs from parameters can be learned on the job, but you should already be familiar &#40;but not necessarily an expert&#41; with the estimation techniques you are looking to employ.</p> <p><strong>Expected Results</strong>: Library functions for performing parameter estimation and inferring properties of differential equation solutions from parameters. Notebooks containing benchmarks determining the effectiveness of various methods and classifying when specific approaches are appropriate will be developed simultaneously.</p> <p><strong>Mentors</strong>: <a href="https://github.com/ChrisRackauckas">Chris Rackauckas</a>, <a href="https://github.com/Vaibhavdixit02">Vaibhav Dixit</a></p> <h2 id=integration_of_fenicsjl_with_dolfin-adjoint_zygotejl_for_scientific_machine_learning ><a href="#integration_of_fenicsjl_with_dolfin-adjoint_zygotejl_for_scientific_machine_learning" class=header-anchor >Integration of FEniCS.jl with dolfin-adjoint &#43; Zygote.jl for Scientific Machine Learning</a></h2> <p>Scientific machine learning requires mixing scientific computing libraries with machine learning. <a href="https://www.stochasticlifestyle.com/the-essential-tools-of-scientific-machine-learning-scientific-ml/">This blog post highlights how the tooling of Julia is fairly advanced in this field</a> compared to alternatives such as Python, but one area that has not been completely worked out is integration of automatic differentiation with partial differential equations. <a href="https://github.com/JuliaDiffEq/FEniCS.jl">FEniCS.jl</a> is a wrapper to the <a href="https://fenicsproject.org/">FEniCS</a> project for finite element solutions of partial differential equations. We would like to augment the Julia wrappers to allow for integration with Julia&#39;s automatic differentiation libraries like <a href="https://github.com/FluxML/Zygote.jl">Zygote.jl</a> by using <a href="http://www.dolfin-adjoint.org/en/release/">dolfin-adjoint</a>. This would require setting up this library for automatic installation for Julia users and writing adjoint passes which utilize this adjoint builder library. It would result in the first total integration between PDEs and neural networks.</p> <p><strong>Recommended Skills</strong>: A basic background in differential equations and Python. Having previous Julia knowledge is preferred but not strictly required.</p> <p><strong>Expected Results</strong>: Efficient and high-quality implementations of adjoints for Zygote.jl over FEniCS.jl functions.</p> <p><strong>Mentors</strong>: <a href="https://github.com/ChrisRackauckas">Chris Rackauckas</a></p> </div> </div> <!-- <div class=page-foot > <div class=copyright > &copy; {{ fill author }}. {{isnotpage /tag/*}}Last modified: {{ fill fd_mtime }}.{{end}} Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>. </div> </div> --> </div> <div class="container footer lighter"> <p>Flux: A Deep Learning Library for the Julia Programming Language </p> <a href="https://twitter.com/FluxML?ref_src=twsrc%5Etfw" class=twitter-follow-button  data-show-count=false >Follow @FluxML</a> <script async src="https://platform.twitter.com/widgets.js" charset=utf-8 ></script> </div> <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script> <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/js/bootstrap.min.js"></script> <script src="/fluxml-franklin/libs/highlight.pack.js"></script> <script>hljs.initHighlightingOnLoad();</script> <script src="/fluxml-franklin//instant.page/1.0.0" type=module  integrity="sha384-6w2SekMzCkuMQ9sEbq0cLviD/yR2HfA/+ekmKiBnFlsoSvb/VmQFSi/umVShadQI"></script>